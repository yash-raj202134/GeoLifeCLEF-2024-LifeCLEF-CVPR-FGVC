{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb6e5de",
   "metadata": {
    "papermill": {
     "duration": 0.008313,
     "end_time": "2024-05-19T18:56:39.640796",
     "exception": false,
     "start_time": "2024-05-19T18:56:39.632483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Simple baseline with Landsat and Bioclimatic Cubes + Sentinel images [0.31626]\n",
    "\n",
    "Following the three provided baselies with different modalities, we have provide a multimodal approch based on \"siamiese\" network with multiple inputs and simple shared \"decoder\". The links for the separated baselines are as follows:\n",
    "\n",
    "- [Baseline with Bioclimatic Cubes [0.25784]](https://www.kaggle.com/code/picekl/baseline-with-bioclimatic-cubes-0-25784)\n",
    "- [Baseline with Landsat Cubes [0.26424]](https://www.kaggle.com/code/picekl/baseline-with-landsat-cubes-0-26424)\n",
    "- [Baseline with Sentinel Images [0.23594]](https://www.kaggle.com/code/picekl/baseline-with-sentinel-images-0-23594)\n",
    "\n",
    "**Considering the significant extent for enhancing performance of this baseline, we encourage you to experiment with various techniques, architectures, losses, etc.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8faeec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T13:30:07.054038Z",
     "iopub.status.busy": "2024-05-01T13:30:07.053659Z",
     "iopub.status.idle": "2024-05-01T13:30:07.058148Z",
     "shell.execute_reply": "2024-05-01T13:30:07.057269Z",
     "shell.execute_reply.started": "2024-05-01T13:30:07.054008Z"
    },
    "papermill": {
     "duration": 0.007344,
     "end_time": "2024-05-19T18:56:39.655845",
     "exception": false,
     "start_time": "2024-05-19T18:56:39.648501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data description\n",
    "\n",
    "## Landsat time series\n",
    "\n",
    "Satellite time series data includes over 20 years of Landsat satellite imagery extracted from [Ecodatacube](https://stac.ecodatacube.eu/).\n",
    "The data was acquired through the Landsat satellite program and pre-processed by Ecodatacube to produce raster files scaled to the entire European continent and projected into a unique CRS.\n",
    "\n",
    "Since the original rasters require a high amount of disk space, we extracted the data points from each spectral band corresponding to all PA and PO locations (i.e., GPS coordinates) and aggregated them in (i) CSV files and (ii) data cubes as tensor objects. Each data point corresponds to the mean value of Landsat's observations at the given location for three months before the given time; e.g., the value of a time series element under column 2012_4 will represent the mean value for that element from October 2012 to December 2012.\n",
    "\n",
    "In this notebook, we will work with just the cubes. The cubes are structured as follows.\n",
    "**Shape**: `(n_bands, n_quarters, n_years)` where:\n",
    "- `n_bands` = 6 comprising [`red`, `green`, `blue`, `nir`, `swir1`, `swir2`]\n",
    "- `n_quarters` = 4 \n",
    "    - *Quarter 1*: December 2 of previous year until March 20 of current year (winter season proxy),\n",
    "    - *Quarter 2*: March 21 until June 24 of current year (spring season proxy),\n",
    "    - *Quarter 3*: June 25 until September 12 of current year (summer season proxy),\n",
    "    - *Quarter 4*: September 13 until December 1 of current year (fall season proxy).\n",
    "- `n_years` = 21 (ranging from 2000 to 2020)\n",
    "\n",
    "The datacubes can simply be loaded as tensors using PyTorch with the following command :\n",
    "\n",
    "```python\n",
    "import torch\n",
    "torch.load('path_to_file.pt')\n",
    "```\n",
    "\n",
    "**References:**\n",
    "- *Traceability (lineage): This dataset is a seasonally aggregated and gapfilled version of the Landsat GLAD analysis-ready data product presented by Potapov et al., 2020 ( https://doi.org/10.3390/rs12030426 ).*\n",
    "- *Scientific methodology: The Landsat GLAD ARD dataset was aggregated and harmonized using the eumap python package (available at https://eumap.readthedocs.io/en/latest/ ). The full process of gapfilling and harmonization is described in detail in Witjes et al., 2022 (in review, preprint available at https://doi.org/10.21203/rs.3.rs-561383/v3 ).*\n",
    "- *Ecodatacube.eu: Analysis-ready open environmental data cube for Europe (https://doi.org/10.21203/rs.3.rs-2277090/v3).*\n",
    "\n",
    "\n",
    "## Bioclimatic time series\n",
    "\n",
    "The Bioclimatic Cubes are created from **four** monthly GeoTIFF CHELSA (https://chelsa-climate.org/timeseries/) time series climatic rasters with a resolution of 30 arc seconds, i.e. approximately 1km. The four variables are the precipitation (pr), maximum- (taxmax), minimum- (tasmin), and mean (tax) daily temperatures per month from January 2000 to June 2019. We provide the data in three forms: (i) raw rasters (GeoTiff images), (ii) CSV file with pre-extracted values for each location, i.e., surveyId, and (iii) data cubes as tensor object (.pt).\n",
    "\n",
    "In this notebook, we will work with just the cubes. The cubes are structured as follows.\n",
    "**Shape**: `(n_year, n_month, n_bio)` where:\n",
    "- `n_year` = 19 (ranging from 2000 to 2018)\n",
    "- `n_month` = 12 (ranging from January 01 to December 12)\n",
    "- `n_bio` = 4 comprising [`pr` (precipitation), `tas` (mean daily air temperature), `tasmin`, `tasmax`]\n",
    "\n",
    "The datacubes can simply be loaded as tensors using PyTorch with the following command :\n",
    "\n",
    "```python\n",
    "import torch\n",
    "torch.load('path_to_file.pt')\n",
    "```\n",
    "\n",
    "**References:**\n",
    "- *Karger, D.N., Conrad, O., Böhner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W., Zimmermann, N.E., Linder, P., Kessler, M. (2017): Climatologies at high resolution for the Earth land surface areas. Scientific Data. 4 170122. https://doi.org/10.1038/sdata.2017.122*\n",
    "\n",
    "- *Karger D.N., Conrad, O., Böhner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W., Zimmermann, N.E, Linder, H.P., Kessler, M. Data from: Climatologies at high resolution for the earth’s land surface areas. Dryad Digital Repository. http://dx.doi.org/doi:10.5061/dryad.kd1d4*\n",
    "\n",
    "\n",
    "## Sentinel Image Patches\n",
    "\n",
    "The Sentinel Image data was acquired through the Sentinel2 satellite program and pre-processed by [Ecodatacube](https://stac.ecodatacube.eu/) to produce raster files scaled to the entire European continent and projected into a unique CRS. We filtered the data in order to pick patches from each spectral band corresponding to a location ((lon, lat) GPS coordinates) and a date matching that of our occurrences', and split them into JPEG files (RGB in 3-channels .jpeg files and NIR in single-channel .jpeg files) with a 128x128 resolution. The images were converted from sentinel uint15 to uint8 by clipping data pixel values over 10000 and applying a gamma correction of 2.5.\n",
    "\n",
    "The data can simply be loaded using the following method:\n",
    "\n",
    "```python\n",
    "def construct_patch_path(output_path, survey_id):\n",
    "    \"\"\"Construct the patch file path based on survey_id as './CD/AB/XXXXABCD.jpeg'\"\"\"\n",
    "    path = output_path\n",
    "    for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "        path = os.path.join(path, d)\n",
    "\n",
    "    path = os.path.join(path, f\"{survey_id}.jpeg\")\n",
    "\n",
    "    return path\n",
    "```\n",
    "\n",
    "**References:**\n",
    "- *Traceability (lineage): The dataset was produced entirely by mosaicking and seasonally aggregating imagery from the Sentinel-2 Level-2A product (https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/product-types/level-2a)*\n",
    "- *Ecodatacube.eu: Analysis-ready open environmental data cube for Europe (https://doi.org/10.21203/rs.3.rs-2277090/v3)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9de26a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:07.29831Z",
     "start_time": "2024-04-30T21:25:05.354584Z"
    },
    "execution": {
     "iopub.execute_input": "2024-05-19T18:56:39.672542Z",
     "iopub.status.busy": "2024-05-19T18:56:39.672158Z",
     "iopub.status.idle": "2024-05-19T18:56:47.747099Z",
     "shell.execute_reply": "2024-05-19T18:56:47.746309Z"
    },
    "papermill": {
     "duration": 8.0861,
     "end_time": "2024-05-19T18:56:47.749429",
     "exception": false,
     "start_time": "2024-05-19T18:56:39.663329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.distributions.beta import Beta\n",
    "os.chdir(\"../../\")\n",
    "DEBUG = False\n",
    "num_classes = 11255 # Number of all unique classes within the PO and PA data.\n",
    "seed = 151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a124c9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\project\\\\GeoLifeCLEF-LifeCLEF-CVPR-FGVC'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a8098",
   "metadata": {
    "papermill": {
     "duration": 0.007445,
     "end_time": "2024-05-19T18:56:47.764838",
     "exception": false,
     "start_time": "2024-05-19T18:56:47.757393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare custom dataset loader\n",
    "\n",
    "We have to slightly update the Dataset to provide the relevant data in the appropriate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46864ba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:32.627928Z",
     "start_time": "2024-04-30T21:25:32.612131Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-19T18:56:47.782082Z",
     "iopub.status.busy": "2024-05-19T18:56:47.781585Z",
     "iopub.status.idle": "2024-05-19T18:56:47.807788Z",
     "shell.execute_reply": "2024-05-19T18:56:47.807103Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.037284,
     "end_time": "2024-05-19T18:56:47.809797",
     "exception": false,
     "start_time": "2024-05-19T18:56:47.772513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def construct_patch_path(data_path, survey_id):\n",
    "    \"\"\"Construct the patch file path based on plot_id as './CD/AB/XXXXABCD.jpeg'\"\"\"\n",
    "    path = data_path\n",
    "    for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "        path = os.path.join(path, d)\n",
    "\n",
    "    path = os.path.join(path, f\"{survey_id}.jpeg\")\n",
    "\n",
    "    return path\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, tab, bioclim_data_dir, landsat_data_dir, sentinel_data_dir, metadata, transform=None):\n",
    "        self.tab = tab\n",
    "        self.transform = transform\n",
    "        self.sentinel_transform = transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "      \n",
    "        self.bioclim_data_dir = bioclim_data_dir\n",
    "        self.landsat_data_dir = landsat_data_dir\n",
    "        self.sentinel_data_dir = sentinel_data_dir\n",
    "        self.metadata = metadata\n",
    "        self.metadata = self.metadata.dropna(subset=\"speciesId\").reset_index(drop=True)\n",
    "        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)\n",
    "        self.label_dict = self.metadata.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "        \n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        tab = torch.Tensor(self.tab[self.tab[\"surveyId\"]==survey_id][features].values[0])\n",
    "        landsat_sample = torch.nan_to_num(torch.load(os.path.join(self.landsat_data_dir, f\"GLC24-PA-train-landsat-time-series_{survey_id}_cube.pt\")))\n",
    "        bioclim_sample = torch.nan_to_num(torch.load(os.path.join(self.bioclim_data_dir, f\"GLC24-PA-train-bioclimatic_monthly_{survey_id}_cube.pt\")))\n",
    "\n",
    "        rgb_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir, survey_id)))\n",
    "        nir_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir.replace(\"rgb\", \"nir\").replace(\"RGB\", \"NIR\"), survey_id)))\n",
    "        sentinel_sample = np.concatenate((rgb_sample, nir_sample[...,None]), axis=2)\n",
    "\n",
    "        species_ids = self.label_dict.get(survey_id, [])  # Get list of species IDs for the survey ID\n",
    "        label = torch.zeros(num_classes)  # Initialize label tensor\n",
    "        for species_id in species_ids:\n",
    "            label_id = species_id\n",
    "            label[label_id] = 1  # Set the corresponding class index to 1 for each species\n",
    "        \n",
    "        if isinstance(landsat_sample, torch.Tensor):\n",
    "            landsat_sample = landsat_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            landsat_sample = landsat_sample.numpy()  # Convert tensor to numpy array\n",
    "            \n",
    "        if isinstance(bioclim_sample, torch.Tensor):\n",
    "            bioclim_sample = bioclim_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            bioclim_sample = bioclim_sample.numpy()  # Convert tensor to numpy array   \n",
    "        \n",
    "        if self.transform:\n",
    "            landsat_sample = self.transform(landsat_sample)\n",
    "            bioclim_sample = self.transform(bioclim_sample)\n",
    "            sentinel_sample = self.sentinel_transform(sentinel_sample)\n",
    "\n",
    "        return tab, landsat_sample, bioclim_sample, sentinel_sample, label, survey_id\n",
    "\n",
    "class TestDataset(TrainDataset):\n",
    "    def __init__(self, tab, bioclim_data_dir, landsat_data_dir, sentinel_data_dir, metadata, transform=None):\n",
    "        self.tab = tab\n",
    "        self.transform = transform\n",
    "        self.sentinel_transform = transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "      \n",
    "        self.bioclim_data_dir = bioclim_data_dir\n",
    "        self.landsat_data_dir = landsat_data_dir\n",
    "        self.sentinel_data_dir = sentinel_data_dir\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        tab = torch.Tensor(self.tab[self.tab[\"surveyId\"]==survey_id][features].values[0])\n",
    "        landsat_sample = torch.nan_to_num(torch.load(os.path.join(self.landsat_data_dir, f\"GLC24-PA-test-landsat_time_series_{survey_id}_cube.pt\")))\n",
    "        bioclim_sample = torch.nan_to_num(torch.load(os.path.join(self.bioclim_data_dir, f\"GLC24-PA-test-bioclimatic_monthly_{survey_id}_cube.pt\")))\n",
    "        \n",
    "        rgb_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir, survey_id)))\n",
    "        nir_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir.replace(\"rgb\", \"nir\").replace(\"RGB\", \"NIR\"), survey_id)))\n",
    "        sentinel_sample = np.concatenate((rgb_sample, nir_sample[...,None]), axis=2)\n",
    "\n",
    "        if isinstance(landsat_sample, torch.Tensor):\n",
    "            landsat_sample = landsat_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            landsat_sample = landsat_sample.numpy()  # Convert tensor to numpy array\n",
    "            \n",
    "        if isinstance(bioclim_sample, torch.Tensor):\n",
    "            bioclim_sample = bioclim_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            bioclim_sample = bioclim_sample.numpy()  # Convert tensor to numpy array   \n",
    "        \n",
    "        if self.transform:\n",
    "            landsat_sample = self.transform(landsat_sample)\n",
    "            bioclim_sample = self.transform(bioclim_sample)\n",
    "            sentinel_sample = self.sentinel_transform(sentinel_sample)\n",
    "\n",
    "        return tab, landsat_sample, bioclim_sample, sentinel_sample, survey_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7c25055",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T18:56:47.826406Z",
     "iopub.status.busy": "2024-05-19T18:56:47.826107Z",
     "iopub.status.idle": "2024-05-19T18:56:47.875529Z",
     "shell.execute_reply": "2024-05-19T18:56:47.874546Z"
    },
    "papermill": {
     "duration": 0.060678,
     "end_time": "2024-05-19T18:56:47.878124",
     "exception": false,
     "start_time": "2024-05-19T18:56:47.817446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    # Set seed for Python's built-in random number generator\n",
    "    torch.manual_seed(seed)\n",
    "    # Set seed for numpy\n",
    "    np.random.seed(seed)\n",
    "    # Set seed for CUDA if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # Set cuDNN's random number generator seed for deterministic behavior\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2df295c",
   "metadata": {
    "papermill": {
     "duration": 0.007811,
     "end_time": "2024-05-19T18:56:47.894190",
     "exception": false,
     "start_time": "2024-05-19T18:56:47.886379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load metadata and prepare data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9093cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T18:56:47.911610Z",
     "iopub.status.busy": "2024-05-19T18:56:47.910720Z",
     "iopub.status.idle": "2024-05-19T18:56:48.862811Z",
     "shell.execute_reply": "2024-05-19T18:56:48.861740Z"
    },
    "papermill": {
     "duration": 0.962938,
     "end_time": "2024-05-19T18:56:48.864960",
     "exception": false,
     "start_time": "2024-05-19T18:56:47.902022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surveyId</th>\n",
       "      <th>Bio1</th>\n",
       "      <th>Bio2</th>\n",
       "      <th>Bio3</th>\n",
       "      <th>Bio4</th>\n",
       "      <th>Bio5</th>\n",
       "      <th>Bio6</th>\n",
       "      <th>Bio7</th>\n",
       "      <th>Bio8</th>\n",
       "      <th>Bio9</th>\n",
       "      <th>...</th>\n",
       "      <th>Soilgrid-bdod</th>\n",
       "      <th>Soilgrid-cec</th>\n",
       "      <th>Soilgrid-cfvo</th>\n",
       "      <th>Soilgrid-clay</th>\n",
       "      <th>Soilgrid-nitrogen</th>\n",
       "      <th>Soilgrid-phh2o</th>\n",
       "      <th>Soilgrid-sand</th>\n",
       "      <th>Soilgrid-silt</th>\n",
       "      <th>Soilgrid-soc</th>\n",
       "      <th>LandCover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>642</td>\n",
       "      <td>2814</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>5717</td>\n",
       "      <td>2934</td>\n",
       "      <td>2720</td>\n",
       "      <td>214</td>\n",
       "      <td>2859</td>\n",
       "      <td>2767</td>\n",
       "      <td>...</td>\n",
       "      <td>114.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>634.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1792</td>\n",
       "      <td>2826</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>7607</td>\n",
       "      <td>2975</td>\n",
       "      <td>2666</td>\n",
       "      <td>309</td>\n",
       "      <td>2922</td>\n",
       "      <td>2780</td>\n",
       "      <td>...</td>\n",
       "      <td>126.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3256</td>\n",
       "      <td>2735</td>\n",
       "      <td>93</td>\n",
       "      <td>3</td>\n",
       "      <td>6168</td>\n",
       "      <td>2876</td>\n",
       "      <td>2611</td>\n",
       "      <td>265</td>\n",
       "      <td>2755</td>\n",
       "      <td>2669</td>\n",
       "      <td>...</td>\n",
       "      <td>109.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>547.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3855</td>\n",
       "      <td>2739</td>\n",
       "      <td>95</td>\n",
       "      <td>3</td>\n",
       "      <td>7129</td>\n",
       "      <td>2880</td>\n",
       "      <td>2583</td>\n",
       "      <td>297</td>\n",
       "      <td>2811</td>\n",
       "      <td>2661</td>\n",
       "      <td>...</td>\n",
       "      <td>112.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>435.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4889</td>\n",
       "      <td>2809</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>5603</td>\n",
       "      <td>2928</td>\n",
       "      <td>2715</td>\n",
       "      <td>213</td>\n",
       "      <td>2854</td>\n",
       "      <td>2797</td>\n",
       "      <td>...</td>\n",
       "      <td>113.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>751.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   surveyId  Bio1  Bio2  Bio3  Bio4  Bio5  Bio6  Bio7  Bio8  Bio9  ...  \\\n",
       "0       642  2814    57     2  5717  2934  2720   214  2859  2767  ...   \n",
       "1      1792  2826   104     3  7607  2975  2666   309  2922  2780  ...   \n",
       "2      3256  2735    93     3  6168  2876  2611   265  2755  2669  ...   \n",
       "3      3855  2739    95     3  7129  2880  2583   297  2811  2661  ...   \n",
       "4      4889  2809    62     2  5603  2928  2715   213  2854  2797  ...   \n",
       "\n",
       "   Soilgrid-bdod  Soilgrid-cec  Soilgrid-cfvo  Soilgrid-clay  \\\n",
       "0          114.0         229.0           96.0           95.0   \n",
       "1          126.0         204.0          156.0          238.0   \n",
       "2          109.0         246.0          193.0          210.0   \n",
       "3          112.0         252.0          195.0          189.0   \n",
       "4          113.0         146.0           86.0           96.0   \n",
       "\n",
       "   Soilgrid-nitrogen  Soilgrid-phh2o  Soilgrid-sand  Soilgrid-silt  \\\n",
       "0              389.0            48.0          634.0          270.0   \n",
       "1              259.0            68.0          349.0          411.0   \n",
       "2              547.0            59.0          370.0          419.0   \n",
       "3              489.0            60.0          375.0          435.0   \n",
       "4              220.0            53.0          751.0          153.0   \n",
       "\n",
       "   Soilgrid-soc  LandCover  \n",
       "0         891.0        9.0  \n",
       "1         311.0       10.0  \n",
       "2         717.0       10.0  \n",
       "3         571.0       10.0  \n",
       "4         421.0        5.0  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_landcover = pd.read_csv(\"Dataset/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/LandCover/GLC24-PA-train-landcover.csv\")\n",
    "train_solidgrids = pd.read_csv(\"Dataset/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/SoilGrids/GLC24-PA-train-soilgrids.csv\")\n",
    "train_humanfootprint = pd.read_csv(\"Dataset/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Human Footprint/GLC24-PA-train-human_footprint.csv\")\n",
    "train_elevation = pd.read_csv(\"Dataset/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Elevation/GLC24-PA-train-elevation.csv\")\n",
    "train_climate = pd.read_csv(\"Dataset/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Climate/Average 1981-2010/GLC24-PA-train-bioclimatic.csv\")\n",
    "\n",
    "train_tab = train_climate.merge(train_elevation,on=\"surveyId\").merge(train_humanfootprint,on=\"surveyId\").merge(train_solidgrids,on=\"surveyId\").merge(train_landcover,on=\"surveyId\")\n",
    "features = list(train_tab.columns)[1:]\n",
    "train_tab = train_tab.fillna(-1).replace(np.inf, -1).replace(-np.inf, -1)\n",
    "\n",
    "test_landcover = pd.read_csv(\"Dataset/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/LandCover/GLC24-PA-test-landcover.csv\")\n",
    "test_solidgrids = pd.read_csv(\"Dataset/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/SoilGrids/GLC24-PA-test-soilgrids.csv\")\n",
    "test_humanfootprint = pd.read_csv(\"Dataset/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Human Footprint/GLC24-PA-test-human_footprint.csv\")\n",
    "test_elevation = pd.read_csv(\"Dataset/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Elevation/GLC24-PA-test-elevation.csv\")\n",
    "test_climate = pd.read_csv(\"Dataset/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Climate/Average 1981-2010/GLC24-PA-test-bioclimatic.csv\")\n",
    "\n",
    "test_tab = test_climate.merge(test_elevation,on=\"surveyId\").merge(test_humanfootprint,on=\"surveyId\").merge(test_solidgrids,on=\"surveyId\").merge(test_landcover,on=\"surveyId\")\n",
    "test_tab = test_tab.fillna(-1).replace(np.inf, -1).replace(-np.inf, -1)\n",
    "test_tab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2404bbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surveyId</th>\n",
       "      <th>Bio1</th>\n",
       "      <th>Bio2</th>\n",
       "      <th>Bio3</th>\n",
       "      <th>Bio4</th>\n",
       "      <th>Bio5</th>\n",
       "      <th>Bio6</th>\n",
       "      <th>Bio7</th>\n",
       "      <th>Bio8</th>\n",
       "      <th>Bio9</th>\n",
       "      <th>...</th>\n",
       "      <th>Soilgrid-bdod</th>\n",
       "      <th>Soilgrid-cec</th>\n",
       "      <th>Soilgrid-cfvo</th>\n",
       "      <th>Soilgrid-clay</th>\n",
       "      <th>Soilgrid-nitrogen</th>\n",
       "      <th>Soilgrid-phh2o</th>\n",
       "      <th>Soilgrid-sand</th>\n",
       "      <th>Soilgrid-silt</th>\n",
       "      <th>Soilgrid-soc</th>\n",
       "      <th>LandCover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>212</td>\n",
       "      <td>2883</td>\n",
       "      <td>68</td>\n",
       "      <td>3</td>\n",
       "      <td>5545</td>\n",
       "      <td>3007</td>\n",
       "      <td>2780</td>\n",
       "      <td>227</td>\n",
       "      <td>2854</td>\n",
       "      <td>2958</td>\n",
       "      <td>...</td>\n",
       "      <td>140.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>222</td>\n",
       "      <td>2815</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>5690</td>\n",
       "      <td>2935</td>\n",
       "      <td>2720</td>\n",
       "      <td>215</td>\n",
       "      <td>2860</td>\n",
       "      <td>2768</td>\n",
       "      <td>...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>609.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>243</td>\n",
       "      <td>2821</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>5150</td>\n",
       "      <td>2910</td>\n",
       "      <td>2741</td>\n",
       "      <td>169</td>\n",
       "      <td>2838</td>\n",
       "      <td>2832</td>\n",
       "      <td>...</td>\n",
       "      <td>115.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>672.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>324</td>\n",
       "      <td>2870</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>5315</td>\n",
       "      <td>2992</td>\n",
       "      <td>2756</td>\n",
       "      <td>236</td>\n",
       "      <td>2810</td>\n",
       "      <td>2938</td>\n",
       "      <td>...</td>\n",
       "      <td>118.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>333</td>\n",
       "      <td>2858</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>5285</td>\n",
       "      <td>2982</td>\n",
       "      <td>2758</td>\n",
       "      <td>224</td>\n",
       "      <td>2827</td>\n",
       "      <td>2927</td>\n",
       "      <td>...</td>\n",
       "      <td>138.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   surveyId  Bio1  Bio2  Bio3  Bio4  Bio5  Bio6  Bio7  Bio8  Bio9  ...  \\\n",
       "0       212  2883    68     3  5545  3007  2780   227  2854  2958  ...   \n",
       "1       222  2815    59     2  5690  2935  2720   215  2860  2768  ...   \n",
       "2       243  2821    26     1  5150  2910  2741   169  2838  2832  ...   \n",
       "3       324  2870    90     3  5315  2992  2756   236  2810  2938  ...   \n",
       "4       333  2858    81     3  5285  2982  2758   224  2827  2927  ...   \n",
       "\n",
       "   Soilgrid-bdod  Soilgrid-cec  Soilgrid-cfvo  Soilgrid-clay  \\\n",
       "0          140.0         214.0          151.0          292.0   \n",
       "1          120.0         225.0          101.0           94.0   \n",
       "2          115.0         261.0           86.0          131.0   \n",
       "3          118.0         179.0          117.0          225.0   \n",
       "4          138.0         233.0          176.0          328.0   \n",
       "\n",
       "   Soilgrid-nitrogen  Soilgrid-phh2o  Soilgrid-sand  Soilgrid-silt  \\\n",
       "0              159.0            73.0          284.0          422.0   \n",
       "1              379.0            58.0          650.0          255.0   \n",
       "2              373.0            54.0          750.0          117.0   \n",
       "3              230.0            57.0          238.0          535.0   \n",
       "4              250.0            71.0          231.0          439.0   \n",
       "\n",
       "   Soilgrid-soc  LandCover  \n",
       "0         176.0        8.0  \n",
       "1         609.0       12.0  \n",
       "2         672.0        5.0  \n",
       "3         302.0       14.0  \n",
       "4         257.0       14.0  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45d34d79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:34.532017Z",
     "start_time": "2024-04-30T21:25:32.615562Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-19T18:56:48.884206Z",
     "iopub.status.busy": "2024-05-19T18:56:48.883462Z",
     "iopub.status.idle": "2024-05-19T18:56:54.575528Z",
     "shell.execute_reply": "2024-05-19T18:56:54.574540Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.703274,
     "end_time": "2024-05-19T18:56:54.577658",
     "exception": false,
     "start_time": "2024-05-19T18:56:48.874384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75638, 13349)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset and DataLoader\n",
    "batch_size = 128\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load Training metadata\n",
    "train_landsat_data_path = \"/Dataset/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-train-landsat_time_series/\"\n",
    "train_bioclim_data_path = \"/Dataset/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-train-bioclimatic_monthly/\"\n",
    "train_sentinel_data_path=\"/Dataset/geolifeclef-2024/PA_Train_SatellitePatches_RGB/pa_train_patches_rgb/\"\n",
    "train_metadata_path = \"Dataset/geolifeclef-2024/GLC24_PA_metadata_train.csv\"\n",
    "\n",
    "train_metadata = pd.read_csv(train_metadata_path)\n",
    "dataset = TrainDataset(train_tab, train_bioclim_data_path, train_landsat_data_path, train_sentinel_data_path, train_metadata, transform=transform)\n",
    "training, validation = random_split(dataset,\n",
    "                                    [int(len(dataset)*0.85), len(dataset)-int(len(dataset)*0.85)],\n",
    "                                    generator=torch.Generator().manual_seed(seed),\n",
    "    )\n",
    "train_loader = DataLoader(training, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(validation, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Load Test metadata\n",
    "test_landsat_data_path = \"/Dataset/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-test-landsat_time_series/\"\n",
    "test_bioclim_data_path = \"/Dataset/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-test-bioclimatic_monthly/\"\n",
    "test_sentinel_data_path = \"/Dataset/geolifeclef-2024/PA_Test_SatellitePatches_RGB/pa_test_patches_rgb/\"\n",
    "test_metadata_path = \"Dataset/geolifeclef-2024/GLC24_PA_metadata_test.csv\"\n",
    "\n",
    "test_metadata = pd.read_csv(test_metadata_path)\n",
    "test_dataset = TestDataset(test_tab, test_bioclim_data_path, test_landsat_data_path, test_sentinel_data_path, test_metadata, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "len(training),len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94ef0a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69884599",
   "metadata": {
    "papermill": {
     "duration": 0.007992,
     "end_time": "2024-05-19T18:56:54.594064",
     "exception": false,
     "start_time": "2024-05-19T18:56:54.586072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define and initialize a Multimodal Model\n",
    "\n",
    "To process multiple inputs with different modalities and formats we use so-call siamiese approach where each modality is processed with different backbone (i.e., encoder). Data encoded into a 1d vector are concatenated and classified with a simple fully connected neural network. Short recap from previous notebooks.\n",
    "- The Landsat cubes have a shape of [6,4,21] (BANDs, QUARTERs, and YEARs).\n",
    "- The Bioclimatic cubes have a shape of [4,19,12] (RASTER-TYPE, YEAR, and MONTH)\n",
    "- The Sentinel Image Patches have a shape od [128, 128, 4] (R, G, B, NIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f1c0e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:31.014067Z",
     "start_time": "2024-04-30T21:25:31.01006Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-19T18:56:54.611681Z",
     "iopub.status.busy": "2024-05-19T18:56:54.610971Z",
     "iopub.status.idle": "2024-05-19T18:56:54.626626Z",
     "shell.execute_reply": "2024-05-19T18:56:54.625824Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026713,
     "end_time": "2024-05-19T18:56:54.628677",
     "exception": false,
     "start_time": "2024-05-19T18:56:54.601964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultimodalEnsemble(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultimodalEnsemble, self).__init__()\n",
    "        self.tab_norm = nn.LayerNorm([len(features)])\n",
    "        self.tab_model = nn.Sequential(nn.Linear(len(features),128),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(128,128),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(128,32),\n",
    "                                      )\n",
    "        \n",
    "        self.landsat_norm = nn.LayerNorm([6,4,21])\n",
    "        self.landsat_model = models.resnet18(weights=None)\n",
    "        # Modify the first convolutional layer to accept 6 channels instead of 3\n",
    "        self.landsat_model.conv1 = nn.Conv2d(6, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.landsat_model.maxpool = nn.Identity()\n",
    "        \n",
    "        self.bioclim_norm = nn.LayerNorm([4,19,12])\n",
    "        self.bioclim_model = models.resnet18(weights=None)\n",
    "        # Modify the first convolutional layer to accept 4 channels instead of 3\n",
    "        self.bioclim_model.conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bioclim_model.maxpool = nn.Identity()\n",
    "        \n",
    "        self.sentinel_model = models.swin_t(weights=\"IMAGENET1K_V1\")\n",
    "        # Modify the first layer to accept 4 channels instead of 3\n",
    "        self.sentinel_model.features[0][0] = nn.Conv2d(4, 96, kernel_size=(4, 4), stride=(4, 4))\n",
    "        self.sentinel_model.head = nn.Identity()\n",
    "        \n",
    "        self.ln0 = nn.LayerNorm(32)\n",
    "        self.ln1 = nn.LayerNorm(1000)\n",
    "        self.ln2 = nn.LayerNorm(1000)\n",
    "        self.ln3 = nn.LayerNorm(768)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2768+32, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.15)\n",
    "        \n",
    "    def forward(self, t, x, y, z):\n",
    "        t = self.tab_norm(t)\n",
    "        t = self.tab_model(t)\n",
    "        t = self.ln0(t)\n",
    "        t = self.dropout(t)\n",
    "        \n",
    "        x = self.landsat_norm(x)\n",
    "        x = self.landsat_model(x)\n",
    "        x = self.ln1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        y = self.bioclim_norm(y)\n",
    "        y = self.bioclim_model(y)\n",
    "        y = self.ln2(y)\n",
    "        y = self.dropout(y)\n",
    "        \n",
    "        z = self.sentinel_model(z)\n",
    "        z = self.ln3(z)\n",
    "        z = self.dropout(z)\n",
    "        \n",
    "        txyz = torch.cat((t, x, y, z), dim=1)\n",
    "        \n",
    "        txyz = self.fc1(txyz).relu()\n",
    "        txyz = self.dropout(txyz)\n",
    "        \n",
    "        out = self.fc2(txyz)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97f75357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:31.611823Z",
     "start_time": "2024-04-30T21:25:31.607373Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-19T18:56:54.645717Z",
     "iopub.status.busy": "2024-05-19T18:56:54.645461Z",
     "iopub.status.idle": "2024-05-19T18:56:57.413163Z",
     "shell.execute_reply": "2024-05-19T18:56:57.412231Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 2.778874,
     "end_time": "2024-05-19T18:56:57.415524",
     "exception": false,
     "start_time": "2024-05-19T18:56:54.636650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = CUDA\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"DEVICE = CUDA\")\n",
    "\n",
    "model = MultimodalEnsemble(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "056faa93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4070 SUPER'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427073f4",
   "metadata": {
    "papermill": {
     "duration": 0.009154,
     "end_time": "2024-05-19T18:56:57.434143",
     "exception": false,
     "start_time": "2024-05-19T18:56:57.424989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training Loop\n",
    "\n",
    "Nothing special, just a standard Pytorch training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d59da59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:32.181927Z",
     "start_time": "2024-04-30T21:25:32.177073Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-19T18:56:57.454598Z",
     "iopub.status.busy": "2024-05-19T18:56:57.453728Z",
     "iopub.status.idle": "2024-05-19T18:56:57.461070Z",
     "shell.execute_reply": "2024-05-19T18:56:57.460198Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.019557,
     "end_time": "2024-05-19T18:56:57.463076",
     "exception": false,
     "start_time": "2024-05-19T18:56:57.443519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 3 if DEBUG else 15\n",
    "#positive_weigh_factor = 1.0\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f91c1297",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-30T21:25:34.536634Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-19T18:56:57.482322Z",
     "iopub.status.busy": "2024-05-19T18:56:57.482067Z",
     "iopub.status.idle": "2024-05-19T20:35:50.314143Z",
     "shell.execute_reply": "2024-05-19T20:35:50.312955Z"
    },
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5932.844537,
     "end_time": "2024-05-19T20:35:50.316641",
     "exception": false,
     "start_time": "2024-05-19T18:56:57.472104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 15 epochs started.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/591 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "print(f\"Training for {num_epochs} epochs started.\")\n",
    "\n",
    "best_val = None\n",
    "best_model = deepcopy(model)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # training \n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    print(\"1\")\n",
    "    for batch_idx, (data0, data1, data2, data3, targets, _) in enumerate(tqdm(train_loader)):\n",
    "        print(\"2\")\n",
    "        data0 = data0.to(device)\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        data3 = data3.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Mixup\n",
    "        if np.random.rand() < 0.4:\n",
    "            lam = torch.tensor(np.random.beta(0.4, 0.4)).to(device)\n",
    "            rand_index = torch.randperm(data0.size()[0]).to(device)\n",
    "            mixed_data0 = lam * data0 + (1 - lam) * data0[rand_index]\n",
    "            mixed_data1 = lam * data1 + (1 - lam) * data1[rand_index]\n",
    "            mixed_data2 = lam * data2 + (1 - lam) * data2[rand_index]\n",
    "            mixed_data3 = lam * data3 + (1 - lam) * data3[rand_index]\n",
    "            targets_a, targets_b = targets, targets[rand_index]\n",
    "            mixed_targets = lam * targets_a + (1 - lam) * targets_b\n",
    "            outputs = model(mixed_data0, mixed_data1, mixed_data2, mixed_data3)\n",
    "            loss = criterion(outputs, mixed_targets)\n",
    "        else:\n",
    "            outputs = model(data0, data1, data2, data3)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total += data1.shape[0]\n",
    "        total_loss += loss.item() * data1.shape[0]\n",
    "        \n",
    "        if DEBUG and batch_idx > 50:\n",
    "            break\n",
    "        \n",
    "    total_loss /= total\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        # validation \n",
    "        vtotal = 0\n",
    "        vtotal_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data0, data1, data2, data3, targets, _) in enumerate(val_loader):\n",
    "                data0 = data0.to(device)\n",
    "                data1 = data1.to(device)\n",
    "                data2 = data2.to(device)\n",
    "                data3 = data3.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = model(data0, data1, data2, data3)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                vtotal += data1.shape[0]\n",
    "                vtotal_loss += loss.item() * data1.shape[0]\n",
    "\n",
    "                if DEBUG and batch_idx > 50:\n",
    "                    break\n",
    "\n",
    "        vtotal_loss /= vtotal\n",
    "\n",
    "        print(f'Epoch {epoch} : train_loss {total_loss:.5f} | val_loss {vtotal_loss:.5f}')\n",
    "\n",
    "        if best_val is None or vtotal_loss < best_val:\n",
    "            best_val = vtotal_loss\n",
    "            best_model = deepcopy(model)\n",
    "\n",
    "# Save the trained model\n",
    "model.eval()\n",
    "torch.save(model.state_dict(), \"models/baseline/last.pth\")\n",
    "best_model.eval()\n",
    "torch.save(best_model.state_dict(), \"models/baseline/best.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf5491a",
   "metadata": {
    "papermill": {
     "duration": 0.77105,
     "end_time": "2024-05-19T20:35:51.797573",
     "exception": false,
     "start_time": "2024-05-19T20:35:51.026523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Val & thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6f1a14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T20:35:53.207311Z",
     "iopub.status.busy": "2024-05-19T20:35:53.206941Z",
     "iopub.status.idle": "2024-05-19T20:35:54.843248Z",
     "shell.execute_reply": "2024-05-19T20:35:54.842448Z"
    },
    "papermill": {
     "duration": 2.34458,
     "end_time": "2024-05-19T20:35:54.845572",
     "exception": false,
     "start_time": "2024-05-19T20:35:52.500992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MultimodalEnsemble(num_classes).to(device)\n",
    "model.load_state_dict(torch.load(\"models/baseline/best.pth\",map_location=device))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba5ed9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T20:35:56.326843Z",
     "iopub.status.busy": "2024-05-19T20:35:56.326140Z",
     "iopub.status.idle": "2024-05-19T20:36:25.707429Z",
     "shell.execute_reply": "2024-05-19T20:36:25.706511Z"
    },
    "papermill": {
     "duration": 30.08581,
     "end_time": "2024-05-19T20:36:25.709781",
     "exception": false,
     "start_time": "2024-05-19T20:35:55.623971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    all_surveyID = []\n",
    "    for batch_idx, (data0, data1, data2, data3, targets, surveyID) in enumerate(tqdm(val_loader)):\n",
    "        data0 = data0.to(device)\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        data3 = data3.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(data0, data1, data2, data3)\n",
    "        predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "        all_predictions.extend(predictions)\n",
    "        all_surveyID.extend(surveyID.numpy())\n",
    "        \n",
    "all_predictions = np.array(all_predictions)\n",
    "all_surveyID = np.array(all_surveyID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967af3d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T20:36:27.158040Z",
     "iopub.status.busy": "2024-05-19T20:36:27.157640Z",
     "iopub.status.idle": "2024-05-19T20:36:49.530378Z",
     "shell.execute_reply": "2024-05-19T20:36:49.529391Z"
    },
    "papermill": {
     "duration": 23.09394,
     "end_time": "2024-05-19T20:36:49.532452",
     "exception": false,
     "start_time": "2024-05-19T20:36:26.438512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pa = pd.read_csv(train_metadata_path)\n",
    "gt = []\n",
    "\n",
    "for surveyId in tqdm(all_surveyID):\n",
    "    gt.append(train_pa[train_pa[\"surveyId\"]==surveyId].speciesId.values.astype(int).tolist())\n",
    "    \n",
    "\n",
    "len(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce80a67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T20:36:51.072371Z",
     "iopub.status.busy": "2024-05-19T20:36:51.072029Z",
     "iopub.status.idle": "2024-05-19T20:37:15.412988Z",
     "shell.execute_reply": "2024-05-19T20:37:15.411924Z"
    },
    "papermill": {
     "duration": 25.075897,
     "end_time": "2024-05-19T20:37:15.416037",
     "exception": false,
     "start_time": "2024-05-19T20:36:50.340140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_pred(pred,gt):\n",
    "    list_f1 = []\n",
    "    for p,g in zip(pred,gt):\n",
    "        sp = set(p)\n",
    "        sg = set(g)\n",
    "        TP = len(list(sp.intersection(sg)))\n",
    "        FP = len(list(sp-sg))\n",
    "        FN = len(list(sg-sp))\n",
    "        f1 = TP/(TP + (FP+FN)/2)\n",
    "        list_f1.append(f1)\n",
    "    return np.mean(list_f1)\n",
    "\n",
    "pred_sorted = np.argsort(-all_predictions, axis=1)\n",
    "best_top_k = 1\n",
    "best_score = 0\n",
    "\n",
    "for k in tqdm(range(1,100)):\n",
    "    top_k = pred_sorted[:, :k].tolist()\n",
    "    score = eval_pred(top_k,gt)\n",
    "    if score > best_score :\n",
    "        best_score = score\n",
    "        best_top_k = k\n",
    "        \n",
    "print(f'best score {best_score:.5f} @ top {best_top_k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe249db8",
   "metadata": {
    "papermill": {
     "duration": 0.79667,
     "end_time": "2024-05-19T20:37:16.951671",
     "exception": false,
     "start_time": "2024-05-19T20:37:16.155001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a344bcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T20:37:18.519257Z",
     "iopub.status.busy": "2024-05-19T20:37:18.518855Z",
     "iopub.status.idle": "2024-05-19T20:37:19.980660Z",
     "shell.execute_reply": "2024-05-19T20:37:19.979699Z"
    },
    "papermill": {
     "duration": 2.26922,
     "end_time": "2024-05-19T20:37:19.983080",
     "exception": false,
     "start_time": "2024-05-19T20:37:17.713860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MultimodalEnsemble(num_classes).to(device)\n",
    "model.load_state_dict(torch.load(\"models/baseline/best.pth\",map_location=device))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d8cd76",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-19T20:37:21.629005Z",
     "iopub.status.busy": "2024-05-19T20:37:21.628185Z",
     "iopub.status.idle": "2024-05-19T20:38:02.777634Z",
     "shell.execute_reply": "2024-05-19T20:38:02.776225Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 41.993015,
     "end_time": "2024-05-19T20:38:02.779746",
     "exception": false,
     "start_time": "2024-05-19T20:37:20.786731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    surveys = []\n",
    "    top_k_indices = None\n",
    "    for batch_idx, (data0, data1, data2, data3, surveyID) in enumerate(tqdm(test_loader)):\n",
    "        data0 = data0.to(device)\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        data3 = data3.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(data0, data1, data2, data3)\n",
    "        predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "        # Sellect top-k values as predictions\n",
    "        top_k = np.argsort(-predictions, axis=1)[:, :best_top_k] \n",
    "        if top_k_indices is None:\n",
    "            top_k_indices = top_k\n",
    "        else:\n",
    "            top_k_indices = np.concatenate((top_k_indices, top_k), axis=0)\n",
    "\n",
    "        surveys.extend(surveyID.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556cbe62",
   "metadata": {
    "papermill": {
     "duration": 0.82666,
     "end_time": "2024-05-19T20:38:04.357656",
     "exception": false,
     "start_time": "2024-05-19T20:38:03.530996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Save prediction file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e26e2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T20:38:05.842544Z",
     "iopub.status.busy": "2024-05-19T20:38:05.841823Z",
     "iopub.status.idle": "2024-05-19T20:38:05.929582Z",
     "shell.execute_reply": "2024-05-19T20:38:05.928613Z"
    },
    "papermill": {
     "duration": 0.832759,
     "end_time": "2024-05-19T20:38:05.931651",
     "exception": false,
     "start_time": "2024-05-19T20:38:05.098892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_concatenated = [' '.join(map(str, row)) for row in top_k_indices]\n",
    "\n",
    "pd.DataFrame(\n",
    "    {'surveyId': surveys,\n",
    "     'predictions': data_concatenated,\n",
    "    }).to_csv(\"research/Baseline_experiments/outputs/baseline-crossval-topk-earlystopping/output.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603fe93f",
   "metadata": {
    "papermill": {
     "duration": 0.739792,
     "end_time": "2024-05-19T20:38:07.488239",
     "exception": false,
     "start_time": "2024-05-19T20:38:06.748447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8171035,
     "sourceId": 64733,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6094.43147,
   "end_time": "2024-05-19T20:38:11.252146",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-19T18:56:36.820676",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
