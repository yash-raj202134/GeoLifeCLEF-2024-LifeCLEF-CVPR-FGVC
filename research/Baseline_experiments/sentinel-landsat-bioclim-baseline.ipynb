{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ca35f6",
   "metadata": {
    "papermill": {
     "duration": 0.00681,
     "end_time": "2024-06-03T06:42:43.129062",
     "exception": false,
     "start_time": "2024-06-03T06:42:43.122252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Simple baseline with Landsat and Bioclimatic Cubes + Sentinel images [0.31626]\n",
    "\n",
    "Following the three provided baselies with different modalities, we have provide a multimodal approch based on \"siamiese\" network with multiple inputs and simple shared \"decoder\". The links for the separated baselines are as follows:\n",
    "\n",
    "- [Baseline with Bioclimatic Cubes [0.25784]](https://www.kaggle.com/code/picekl/baseline-with-bioclimatic-cubes-0-25784)\n",
    "- [Baseline with Landsat Cubes [0.26424]](https://www.kaggle.com/code/picekl/baseline-with-landsat-cubes-0-26424)\n",
    "- [Baseline with Sentinel Images [0.23594]](https://www.kaggle.com/code/picekl/baseline-with-sentinel-images-0-23594)\n",
    "\n",
    "**Considering the significant extent for enhancing performance of this baseline, we encourage you to experiment with various techniques, architectures, losses, etc.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d23975d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T13:30:07.054038Z",
     "iopub.status.busy": "2024-05-01T13:30:07.053659Z",
     "iopub.status.idle": "2024-05-01T13:30:07.058148Z",
     "shell.execute_reply": "2024-05-01T13:30:07.057269Z",
     "shell.execute_reply.started": "2024-05-01T13:30:07.054008Z"
    },
    "papermill": {
     "duration": 0.00594,
     "end_time": "2024-06-03T06:42:43.141470",
     "exception": false,
     "start_time": "2024-06-03T06:42:43.135530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data description\n",
    "\n",
    "## Landsat time series\n",
    "\n",
    "Satellite time series data includes over 20 years of Landsat satellite imagery extracted from [Ecodatacube](https://stac.ecodatacube.eu/).\n",
    "The data was acquired through the Landsat satellite program and pre-processed by Ecodatacube to produce raster files scaled to the entire European continent and projected into a unique CRS.\n",
    "\n",
    "Since the original rasters require a high amount of disk space, we extracted the data points from each spectral band corresponding to all PA and PO locations (i.e., GPS coordinates) and aggregated them in (i) CSV files and (ii) data cubes as tensor objects. Each data point corresponds to the mean value of Landsat's observations at the given location for three months before the given time; e.g., the value of a time series element under column 2012_4 will represent the mean value for that element from October 2012 to December 2012.\n",
    "\n",
    "In this notebook, we will work with just the cubes. The cubes are structured as follows.\n",
    "**Shape**: `(n_bands, n_quarters, n_years)` where:\n",
    "- `n_bands` = 6 comprising [`red`, `green`, `blue`, `nir`, `swir1`, `swir2`]\n",
    "- `n_quarters` = 4 \n",
    "    - *Quarter 1*: December 2 of previous year until March 20 of current year (winter season proxy),\n",
    "    - *Quarter 2*: March 21 until June 24 of current year (spring season proxy),\n",
    "    - *Quarter 3*: June 25 until September 12 of current year (summer season proxy),\n",
    "    - *Quarter 4*: September 13 until December 1 of current year (fall season proxy).\n",
    "- `n_years` = 21 (ranging from 2000 to 2020)\n",
    "\n",
    "The datacubes can simply be loaded as tensors using PyTorch with the following command :\n",
    "\n",
    "```python\n",
    "import torch\n",
    "torch.load('path_to_file.pt')\n",
    "```\n",
    "\n",
    "**References:**\n",
    "- *Traceability (lineage): This dataset is a seasonally aggregated and gapfilled version of the Landsat GLAD analysis-ready data product presented by Potapov et al., 2020 ( https://doi.org/10.3390/rs12030426 ).*\n",
    "- *Scientific methodology: The Landsat GLAD ARD dataset was aggregated and harmonized using the eumap python package (available at https://eumap.readthedocs.io/en/latest/ ). The full process of gapfilling and harmonization is described in detail in Witjes et al., 2022 (in review, preprint available at https://doi.org/10.21203/rs.3.rs-561383/v3 ).*\n",
    "- *Ecodatacube.eu: Analysis-ready open environmental data cube for Europe (https://doi.org/10.21203/rs.3.rs-2277090/v3).*\n",
    "\n",
    "\n",
    "## Bioclimatic time series\n",
    "\n",
    "The Bioclimatic Cubes are created from **four** monthly GeoTIFF CHELSA (https://chelsa-climate.org/timeseries/) time series climatic rasters with a resolution of 30 arc seconds, i.e. approximately 1km. The four variables are the precipitation (pr), maximum- (taxmax), minimum- (tasmin), and mean (tax) daily temperatures per month from January 2000 to June 2019. We provide the data in three forms: (i) raw rasters (GeoTiff images), (ii) CSV file with pre-extracted values for each location, i.e., surveyId, and (iii) data cubes as tensor object (.pt).\n",
    "\n",
    "In this notebook, we will work with just the cubes. The cubes are structured as follows.\n",
    "**Shape**: `(n_year, n_month, n_bio)` where:\n",
    "- `n_year` = 19 (ranging from 2000 to 2018)\n",
    "- `n_month` = 12 (ranging from January 01 to December 12)\n",
    "- `n_bio` = 4 comprising [`pr` (precipitation), `tas` (mean daily air temperature), `tasmin`, `tasmax`]\n",
    "\n",
    "The datacubes can simply be loaded as tensors using PyTorch with the following command :\n",
    "\n",
    "```python\n",
    "import torch\n",
    "torch.load('path_to_file.pt')\n",
    "```\n",
    "\n",
    "**References:**\n",
    "- *Karger, D.N., Conrad, O., Böhner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W., Zimmermann, N.E., Linder, P., Kessler, M. (2017): Climatologies at high resolution for the Earth land surface areas. Scientific Data. 4 170122. https://doi.org/10.1038/sdata.2017.122*\n",
    "\n",
    "- *Karger D.N., Conrad, O., Böhner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W., Zimmermann, N.E, Linder, H.P., Kessler, M. Data from: Climatologies at high resolution for the earth’s land surface areas. Dryad Digital Repository. http://dx.doi.org/doi:10.5061/dryad.kd1d4*\n",
    "\n",
    "\n",
    "## Sentinel Image Patches\n",
    "\n",
    "The Sentinel Image data was acquired through the Sentinel2 satellite program and pre-processed by [Ecodatacube](https://stac.ecodatacube.eu/) to produce raster files scaled to the entire European continent and projected into a unique CRS. We filtered the data in order to pick patches from each spectral band corresponding to a location ((lon, lat) GPS coordinates) and a date matching that of our occurrences', and split them into JPEG files (RGB in 3-channels .jpeg files and NIR in single-channel .jpeg files) with a 128x128 resolution. The images were converted from sentinel uint15 to uint8 by clipping data pixel values over 10000 and applying a gamma correction of 2.5.\n",
    "\n",
    "The data can simply be loaded using the following method:\n",
    "\n",
    "```python\n",
    "def construct_patch_path(output_path, survey_id):\n",
    "    \"\"\"Construct the patch file path based on survey_id as './CD/AB/XXXXABCD.jpeg'\"\"\"\n",
    "    path = output_path\n",
    "    for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "        path = os.path.join(path, d)\n",
    "\n",
    "    path = os.path.join(path, f\"{survey_id}.jpeg\")\n",
    "\n",
    "    return path\n",
    "```\n",
    "\n",
    "**References:**\n",
    "- *Traceability (lineage): The dataset was produced entirely by mosaicking and seasonally aggregating imagery from the Sentinel-2 Level-2A product (https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/product-types/level-2a)*\n",
    "- *Ecodatacube.eu: Analysis-ready open environmental data cube for Europe (https://doi.org/10.21203/rs.3.rs-2277090/v3)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9bfe1aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:07.29831Z",
     "start_time": "2024-04-30T21:25:05.354584Z"
    },
    "execution": {
     "iopub.execute_input": "2024-06-03T06:42:43.155918Z",
     "iopub.status.busy": "2024-06-03T06:42:43.155205Z",
     "iopub.status.idle": "2024-06-03T06:42:50.319845Z",
     "shell.execute_reply": "2024-06-03T06:42:50.319038Z"
    },
    "papermill": {
     "duration": 7.174491,
     "end_time": "2024-06-03T06:42:50.322296",
     "exception": false,
     "start_time": "2024-06-03T06:42:43.147805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "# /kaggle/input/geolifeclef-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44254ea9",
   "metadata": {
    "papermill": {
     "duration": 0.00612,
     "end_time": "2024-06-03T06:42:50.335232",
     "exception": false,
     "start_time": "2024-06-03T06:42:50.329112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare custom dataset loader\n",
    "\n",
    "We have to slightly update the Dataset to provide the relevant data in the appropriate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e141802c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:32.627928Z",
     "start_time": "2024-04-30T21:25:32.612131Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-03T06:42:50.350380Z",
     "iopub.status.busy": "2024-06-03T06:42:50.349055Z",
     "iopub.status.idle": "2024-06-03T06:42:50.374150Z",
     "shell.execute_reply": "2024-06-03T06:42:50.373262Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.034928,
     "end_time": "2024-06-03T06:42:50.376225",
     "exception": false,
     "start_time": "2024-06-03T06:42:50.341297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def construct_patch_path(data_path, survey_id):\n",
    "    \"\"\"Construct the patch file path based on plot_id as './CD/AB/XXXXABCD.jpeg'\"\"\"\n",
    "    path = data_path\n",
    "    for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "        path = os.path.join(path, d)\n",
    "\n",
    "    path = os.path.join(path, f\"{survey_id}.jpeg\")\n",
    "\n",
    "    return path\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, bioclim_data_dir, landsat_data_dir, sentinel_data_dir, metadata, transform=None):\n",
    "        self.transform = transform\n",
    "        self.sentinel_transform = transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "      \n",
    "        self.bioclim_data_dir = bioclim_data_dir\n",
    "        self.landsat_data_dir = landsat_data_dir\n",
    "        self.sentinel_data_dir = sentinel_data_dir\n",
    "        self.metadata = metadata\n",
    "        self.metadata = self.metadata.dropna(subset=\"speciesId\").reset_index(drop=True)\n",
    "        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)\n",
    "        self.label_dict = self.metadata.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "        \n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        \n",
    "        landsat_sample = torch.nan_to_num(torch.load(os.path.join(self.landsat_data_dir, f\"GLC24-PA-train-landsat-time-series_{survey_id}_cube.pt\")))\n",
    "        bioclim_sample = torch.nan_to_num(torch.load(os.path.join(self.bioclim_data_dir, f\"GLC24-PA-train-bioclimatic_monthly_{survey_id}_cube.pt\")))\n",
    "\n",
    "        rgb_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir, survey_id)))\n",
    "        nir_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir.replace(\"rgb\", \"nir\").replace(\"RGB\", \"NIR\"), survey_id)))\n",
    "        sentinel_sample = np.concatenate((rgb_sample, nir_sample[...,None]), axis=2)\n",
    "\n",
    "        species_ids = self.label_dict.get(survey_id, [])  # Get list of species IDs for the survey ID\n",
    "        num_classes = 11255\n",
    "        label = torch.zeros(num_classes)  # Initialize label tensor\n",
    "        for species_id in species_ids:\n",
    "            label_id = species_id\n",
    "            label[label_id] = 1  # Set the corresponding class index to 1 for each species\n",
    "        \n",
    "        if isinstance(landsat_sample, torch.Tensor):\n",
    "            landsat_sample = landsat_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            landsat_sample = landsat_sample.numpy()  # Convert tensor to numpy array\n",
    "            \n",
    "        if isinstance(bioclim_sample, torch.Tensor):\n",
    "            bioclim_sample = bioclim_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            bioclim_sample = bioclim_sample.numpy()  # Convert tensor to numpy array   \n",
    "        \n",
    "        if self.transform:\n",
    "            landsat_sample = self.transform(landsat_sample)\n",
    "            bioclim_sample = self.transform(bioclim_sample)\n",
    "            sentinel_sample = self.sentinel_transform(sentinel_sample)\n",
    "\n",
    "        return landsat_sample, bioclim_sample, sentinel_sample, label, survey_id\n",
    "\n",
    "class TestDataset(TrainDataset):\n",
    "    def __init__(self, bioclim_data_dir, landsat_data_dir, sentinel_data_dir, metadata, transform=None):\n",
    "        self.transform = transform\n",
    "        self.sentinel_transform = transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "      \n",
    "        self.bioclim_data_dir = bioclim_data_dir\n",
    "        self.landsat_data_dir = landsat_data_dir\n",
    "        self.sentinel_data_dir = sentinel_data_dir\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        landsat_sample = torch.nan_to_num(torch.load(os.path.join(self.landsat_data_dir, f\"GLC24-PA-test-landsat_time_series_{survey_id}_cube.pt\")))\n",
    "        bioclim_sample = torch.nan_to_num(torch.load(os.path.join(self.bioclim_data_dir, f\"GLC24-PA-test-bioclimatic_monthly_{survey_id}_cube.pt\")))\n",
    "        \n",
    "        rgb_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir, survey_id)))\n",
    "        nir_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir.replace(\"rgb\", \"nir\").replace(\"RGB\", \"NIR\"), survey_id)))\n",
    "        sentinel_sample = np.concatenate((rgb_sample, nir_sample[...,None]), axis=2)\n",
    "\n",
    "        if isinstance(landsat_sample, torch.Tensor):\n",
    "            landsat_sample = landsat_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            landsat_sample = landsat_sample.numpy()  # Convert tensor to numpy array\n",
    "            \n",
    "        if isinstance(bioclim_sample, torch.Tensor):\n",
    "            bioclim_sample = bioclim_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            bioclim_sample = bioclim_sample.numpy()  # Convert tensor to numpy array   \n",
    "        \n",
    "        if self.transform:\n",
    "            landsat_sample = self.transform(landsat_sample)\n",
    "            bioclim_sample = self.transform(bioclim_sample)\n",
    "            sentinel_sample = self.sentinel_transform(sentinel_sample)\n",
    "\n",
    "        return landsat_sample, bioclim_sample, sentinel_sample, survey_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66199a74",
   "metadata": {
    "papermill": {
     "duration": 0.006191,
     "end_time": "2024-06-03T06:42:50.388817",
     "exception": false,
     "start_time": "2024-06-03T06:42:50.382626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load metadata and prepare data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec6c3a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:34.532017Z",
     "start_time": "2024-04-30T21:25:32.615562Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-03T06:42:50.403061Z",
     "iopub.status.busy": "2024-06-03T06:42:50.402330Z",
     "iopub.status.idle": "2024-06-03T06:42:56.559719Z",
     "shell.execute_reply": "2024-06-03T06:42:56.558881Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 6.16703,
     "end_time": "2024-06-03T06:42:56.562035",
     "exception": false,
     "start_time": "2024-06-03T06:42:50.395005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "batch_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load Training metadata\n",
    "train_landsat_data_path = \"/kaggle/input/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-train-landsat_time_series/\"\n",
    "train_bioclim_data_path = \"/kaggle/input/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-train-bioclimatic_monthly/\"\n",
    "train_sentinel_data_path=\"/kaggle/input/geolifeclef-2024/PA_Train_SatellitePatches_RGB/pa_train_patches_rgb/\"\n",
    "train_metadata_path = \"/kaggle/input/geolifeclef-2024/GLC24_PA_metadata_train.csv\"\n",
    "\n",
    "train_metadata = pd.read_csv(train_metadata_path)\n",
    "dataset_alpine = TrainDataset(train_bioclim_data_path, train_landsat_data_path, train_sentinel_data_path, train_metadata, transform=transform)\n",
    "train_loader = DataLoader(dataset_alpine, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Load Test metadata\n",
    "test_landsat_data_path = \"/kaggle/input/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-test-landsat_time_series/\"\n",
    "test_bioclim_data_path = \"/kaggle/input/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-test-bioclimatic_monthly/\"\n",
    "test_sentinel_data_path = \"/kaggle/input/geolifeclef-2024/PA_Test_SatellitePatches_RGB/pa_test_patches_rgb/\"\n",
    "test_metadata_path = \"/kaggle/input/geolifeclef-2024/GLC24_PA_metadata_test.csv\"\n",
    "\n",
    "test_metadata = pd.read_csv(test_metadata_path)\n",
    "test_dataset = TestDataset(test_bioclim_data_path, test_landsat_data_path, test_sentinel_data_path, test_metadata, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93716050",
   "metadata": {
    "papermill": {
     "duration": 0.006003,
     "end_time": "2024-06-03T06:42:56.574533",
     "exception": false,
     "start_time": "2024-06-03T06:42:56.568530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define and initialize a Multimodal Model\n",
    "\n",
    "To process multiple inputs with different modalities and formats we use so-call siamiese approach where each modality is processed with different backbone (i.e., encoder). Data encoded into a 1d vector are concatenated and classified with a simple fully connected neural network. Short recap from previous notebooks.\n",
    "- The Landsat cubes have a shape of [6,4,21] (BANDs, QUARTERs, and YEARs).\n",
    "- The Bioclimatic cubes have a shape of [4,19,12] (RASTER-TYPE, YEAR, and MONTH)\n",
    "- The Sentinel Image Patches have a shape od [128, 128, 4] (R, G, B, NIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74fdf27c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:31.014067Z",
     "start_time": "2024-04-30T21:25:31.01006Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-03T06:42:56.588456Z",
     "iopub.status.busy": "2024-06-03T06:42:56.588079Z",
     "iopub.status.idle": "2024-06-03T06:42:56.603491Z",
     "shell.execute_reply": "2024-06-03T06:42:56.602651Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.024842,
     "end_time": "2024-06-03T06:42:56.605512",
     "exception": false,
     "start_time": "2024-06-03T06:42:56.580670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultimodalEnsemble(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultimodalEnsemble, self).__init__()\n",
    "        \n",
    "        self.landsat_norm = nn.LayerNorm([6,4,21])\n",
    "        self.landsat_model = models.resnet18(weights=None)\n",
    "        # Modify the first convolutional layer to accept 6 channels instead of 3\n",
    "        self.landsat_model.conv1 = nn.Conv2d(6, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.landsat_model.maxpool = nn.Identity()\n",
    "        \n",
    "        self.bioclim_norm = nn.LayerNorm([4,19,12])\n",
    "        self.bioclim_model = models.resnet18(weights=None)  \n",
    "        # Modify the first convolutional layer to accept 4 channels instead of 3\n",
    "        self.bioclim_model.conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bioclim_model.maxpool = nn.Identity()\n",
    "        \n",
    "        self.sentinel_model = models.swin_t(weights=\"IMAGENET1K_V1\")\n",
    "        # Modify the first layer to accept 4 channels instead of 3\n",
    "        self.sentinel_model.features[0][0] = nn.Conv2d(4, 96, kernel_size=(4, 4), stride=(4, 4))\n",
    "        self.sentinel_model.head = nn.Identity()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(1000)\n",
    "        self.ln2 = nn.LayerNorm(1000)\n",
    "        self.fc1 = nn.Linear(2768, 4096)\n",
    "        self.fc2 = nn.Linear(4096, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, x, y, z):\n",
    "        \n",
    "        x = self.landsat_norm(x)\n",
    "        x = self.landsat_model(x)\n",
    "        x = self.ln1(x)\n",
    "        \n",
    "        y = self.bioclim_norm(y)\n",
    "        y = self.bioclim_model(y)\n",
    "        y = self.ln2(y)\n",
    "        \n",
    "        z = self.sentinel_model(z)\n",
    "        \n",
    "        xyz = torch.cat((x, y, z), dim=1)\n",
    "        xyz = self.fc1(xyz)\n",
    "        xyz = self.dropout(xyz)\n",
    "        out = self.fc2(xyz)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a44e6fa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T06:42:56.620804Z",
     "iopub.status.busy": "2024-06-03T06:42:56.620192Z",
     "iopub.status.idle": "2024-06-03T06:42:56.678668Z",
     "shell.execute_reply": "2024-06-03T06:42:56.677826Z"
    },
    "papermill": {
     "duration": 0.069419,
     "end_time": "2024-06-03T06:42:56.681190",
     "exception": false,
     "start_time": "2024-06-03T06:42:56.611771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    # Set seed for Python's built-in random number generator\n",
    "    torch.manual_seed(seed)\n",
    "    # Set seed for numpy\n",
    "    np.random.seed(seed)\n",
    "    # Set seed for CUDA if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # Set cuDNN's random number generator seed for deterministic behavior\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "280d7234",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:31.611823Z",
     "start_time": "2024-04-30T21:25:31.607373Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-03T06:42:56.695440Z",
     "iopub.status.busy": "2024-06-03T06:42:56.695104Z",
     "iopub.status.idle": "2024-06-03T06:42:59.845863Z",
     "shell.execute_reply": "2024-06-03T06:42:59.845001Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3.160883,
     "end_time": "2024-06-03T06:42:59.848420",
     "exception": false,
     "start_time": "2024-06-03T06:42:56.687537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to /root/.cache/torch/hub/checkpoints/swin_t-704ceda3.pth\n",
      "100%|██████████| 108M/108M [00:00<00:00, 131MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"DEVICE = CUDA\")\n",
    "\n",
    "num_classes = 11255 # Number of all unique classes within the PO and PA data.\n",
    "model = MultimodalEnsemble(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee240ee6",
   "metadata": {
    "papermill": {
     "duration": 0.007394,
     "end_time": "2024-06-03T06:42:59.863852",
     "exception": false,
     "start_time": "2024-06-03T06:42:59.856458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training Loop\n",
    "\n",
    "Nothing special, just a standard Pytorch training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f68ef00b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:32.181927Z",
     "start_time": "2024-04-30T21:25:32.177073Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-03T06:42:59.880675Z",
     "iopub.status.busy": "2024-06-03T06:42:59.879796Z",
     "iopub.status.idle": "2024-06-03T06:42:59.888424Z",
     "shell.execute_reply": "2024-06-03T06:42:59.887464Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.019306,
     "end_time": "2024-06-03T06:42:59.890664",
     "exception": false,
     "start_time": "2024-06-03T06:42:59.871358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.5000e-04.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.00025\n",
    "num_epochs = 10\n",
    "positive_weigh_factor = 1.0\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=25, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e83cd31e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-30T21:25:34.536634Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-03T06:42:59.907282Z",
     "iopub.status.busy": "2024-06-03T06:42:59.906703Z",
     "iopub.status.idle": "2024-06-03T08:05:46.423420Z",
     "shell.execute_reply": "2024-06-03T08:05:46.422445Z"
    },
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 4966.527725,
     "end_time": "2024-06-03T08:05:46.425871",
     "exception": false,
     "start_time": "2024-06-03T06:42:59.898146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epochs started.\n",
      "Epoch 1/10, Batch 0/1391, Loss: 0.7076259255409241\n",
      "Epoch 1/10, Batch 278/1391, Loss: 0.005654838401824236\n",
      "Epoch 1/10, Batch 556/1391, Loss: 0.0052689905278384686\n",
      "Epoch 1/10, Batch 834/1391, Loss: 0.004570360761135817\n",
      "Epoch 1/10, Batch 1112/1391, Loss: 0.004214102867990732\n",
      "Epoch 1/10, Batch 1390/1391, Loss: 0.004991599824279547\n",
      "Adjusting learning rate of group 0 to 2.4901e-04.\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 1, 'verbose': True, '_step_count': 2, '_get_lr_called_within_step': False, '_last_lr': [0.00024901433766430975]}\n",
      "Epoch 2/10, Batch 0/1391, Loss: 0.00470429752022028\n",
      "Epoch 2/10, Batch 278/1391, Loss: 0.004218931309878826\n",
      "Epoch 2/10, Batch 556/1391, Loss: 0.004473183769732714\n",
      "Epoch 2/10, Batch 834/1391, Loss: 0.004549738019704819\n",
      "Epoch 2/10, Batch 1112/1391, Loss: 0.004546815529465675\n",
      "Epoch 2/10, Batch 1390/1391, Loss: 0.004234988242387772\n",
      "Adjusting learning rate of group 0 to 2.4607e-04.\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 2, 'verbose': True, '_step_count': 3, '_get_lr_called_within_step': False, '_last_lr': [0.0002460728951410789]}\n",
      "Epoch 3/10, Batch 0/1391, Loss: 0.004403520841151476\n",
      "Epoch 3/10, Batch 278/1391, Loss: 0.004186150152236223\n",
      "Epoch 3/10, Batch 556/1391, Loss: 0.004512460436671972\n",
      "Epoch 3/10, Batch 834/1391, Loss: 0.004742731340229511\n",
      "Epoch 3/10, Batch 1112/1391, Loss: 0.004404635168612003\n",
      "Epoch 3/10, Batch 1390/1391, Loss: 0.005062307696789503\n",
      "Adjusting learning rate of group 0 to 2.4122e-04.\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 3, 'verbose': True, '_step_count': 4, '_get_lr_called_within_step': False, '_last_lr': [0.00024122206073603144]}\n",
      "Epoch 4/10, Batch 0/1391, Loss: 0.0040080351755023\n",
      "Epoch 4/10, Batch 278/1391, Loss: 0.004159296862781048\n",
      "Epoch 4/10, Batch 556/1391, Loss: 0.004321608226746321\n",
      "Epoch 4/10, Batch 834/1391, Loss: 0.004085476975888014\n",
      "Epoch 4/10, Batch 1112/1391, Loss: 0.003978722263127565\n",
      "Epoch 4/10, Batch 1390/1391, Loss: 0.004366511479020119\n",
      "Adjusting learning rate of group 0 to 2.3454e-04.\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 4, 'verbose': True, '_step_count': 5, '_get_lr_called_within_step': False, '_last_lr': [0.00023453833500548295]}\n",
      "Epoch 5/10, Batch 0/1391, Loss: 0.003624669974669814\n",
      "Epoch 5/10, Batch 278/1391, Loss: 0.004295232705771923\n",
      "Epoch 5/10, Batch 556/1391, Loss: 0.004129088018089533\n",
      "Epoch 5/10, Batch 834/1391, Loss: 0.004402792546898127\n",
      "Epoch 5/10, Batch 1112/1391, Loss: 0.003789209993556142\n",
      "Epoch 5/10, Batch 1390/1391, Loss: 0.0039804005064070225\n",
      "Adjusting learning rate of group 0 to 2.2613e-04.\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 5, 'verbose': True, '_step_count': 6, '_get_lr_called_within_step': False, '_last_lr': [0.00022612712429686844]}\n",
      "Epoch 6/10, Batch 0/1391, Loss: 0.003774699755012989\n",
      "Epoch 6/10, Batch 278/1391, Loss: 0.0036435106303542852\n",
      "Epoch 6/10, Batch 556/1391, Loss: 0.004326288588345051\n",
      "Epoch 6/10, Batch 834/1391, Loss: 0.004002690780907869\n",
      "Epoch 6/10, Batch 1112/1391, Loss: 0.0038642261642962694\n",
      "Epoch 6/10, Batch 1390/1391, Loss: 0.0038263914175331593\n",
      "Adjusting learning rate of group 0 to 2.1612e-04.\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 6, 'verbose': True, '_step_count': 7, '_get_lr_called_within_step': False, '_last_lr': [0.00021612107842767644]}\n",
      "Epoch 7/10, Batch 0/1391, Loss: 0.0038647442124783993\n",
      "Epoch 7/10, Batch 278/1391, Loss: 0.0038925562985241413\n",
      "Epoch 7/10, Batch 556/1391, Loss: 0.0034358245320618153\n",
      "Epoch 7/10, Batch 834/1391, Loss: 0.0039037438109517097\n",
      "Epoch 7/10, Batch 1112/1391, Loss: 0.00399558013305068\n",
      "Epoch 7/10, Batch 1390/1391, Loss: 0.003660985268652439\n",
      "Adjusting learning rate of group 0 to 2.0468e-04.\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 7, 'verbose': True, '_step_count': 8, '_get_lr_called_within_step': False, '_last_lr': [0.0002046779987185862]}\n",
      "Epoch 8/10, Batch 0/1391, Loss: 0.0036018877290189266\n",
      "Epoch 8/10, Batch 278/1391, Loss: 0.0037534208968281746\n",
      "Epoch 8/10, Batch 556/1391, Loss: 0.004050590563565493\n",
      "Epoch 8/10, Batch 834/1391, Loss: 0.0036480033304542303\n",
      "Epoch 8/10, Batch 1112/1391, Loss: 0.004224952310323715\n",
      "Epoch 8/10, Batch 1390/1391, Loss: 0.004487078636884689\n",
      "Adjusting learning rate of group 0 to 1.9198e-04.\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 8, 'verbose': True, '_step_count': 9, '_get_lr_called_within_step': False, '_last_lr': [0.00019197834937237455]}\n",
      "Epoch 9/10, Batch 0/1391, Loss: 0.0037154771853238344\n",
      "Epoch 9/10, Batch 278/1391, Loss: 0.0036816562060266733\n",
      "Epoch 9/10, Batch 556/1391, Loss: 0.003497940255329013\n",
      "Epoch 9/10, Batch 834/1391, Loss: 0.0038332671392709017\n",
      "Epoch 9/10, Batch 1112/1391, Loss: 0.003401717636734247\n",
      "Epoch 9/10, Batch 1390/1391, Loss: 0.003910350613296032\n",
      "Adjusting learning rate of group 0 to 1.7822e-04.\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 9, 'verbose': True, '_step_count': 10, '_get_lr_called_within_step': False, '_last_lr': [0.00017822241144563406]}\n",
      "Epoch 10/10, Batch 0/1391, Loss: 0.0031032178085297346\n",
      "Epoch 10/10, Batch 278/1391, Loss: 0.0038909490685909986\n",
      "Epoch 10/10, Batch 556/1391, Loss: 0.0032452947925776243\n",
      "Epoch 10/10, Batch 834/1391, Loss: 0.0034234062768518925\n",
      "Epoch 10/10, Batch 1112/1391, Loss: 0.003675584914162755\n",
      "Epoch 10/10, Batch 1390/1391, Loss: 0.003486430272459984\n",
      "Adjusting learning rate of group 0 to 1.6363e-04.\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0, 'base_lrs': [0.00025], 'last_epoch': 10, 'verbose': True, '_step_count': 11, '_get_lr_called_within_step': False, '_last_lr': [0.0001636271242968684]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training for {num_epochs} epochs started.\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data1, data2, data3, targets, _) in enumerate(train_loader):\n",
    "\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        data3 = data3.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data1, data2, data3)\n",
    "\n",
    "        pos_weight = targets*positive_weigh_factor  # All positive weights are equal to 10\n",
    "        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 278 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\"Scheduler:\",scheduler.state_dict())\n",
    "\n",
    "# Save the trained model\n",
    "model.eval()\n",
    "torch.save(model.state_dict(), \"multimodal-model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba2c55c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T08:05:46.451875Z",
     "iopub.status.busy": "2024-06-03T08:05:46.451568Z",
     "iopub.status.idle": "2024-06-03T08:05:46.458304Z",
     "shell.execute_reply": "2024-06-03T08:05:46.457491Z"
    },
    "papermill": {
     "duration": 0.022081,
     "end_time": "2024-06-03T08:05:46.460521",
     "exception": false,
     "start_time": "2024-06-03T08:05:46.438440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281b6f1",
   "metadata": {
    "papermill": {
     "duration": 0.0118,
     "end_time": "2024-06-03T08:05:46.484411",
     "exception": false,
     "start_time": "2024-06-03T08:05:46.472611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Loop\n",
    "\n",
    "Again, nothing special, just a standard inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "501254b8",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-03T08:05:46.509520Z",
     "iopub.status.busy": "2024-06-03T08:05:46.509235Z",
     "iopub.status.idle": "2024-06-03T08:06:34.799555Z",
     "shell.execute_reply": "2024-06-03T08:06:34.798309Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 48.306111,
     "end_time": "2024-06-03T08:06:34.802197",
     "exception": false,
     "start_time": "2024-06-03T08:05:46.496086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    surveys = []\n",
    "    top_k_indices = None\n",
    "    for batch_idx, (data1, data2, data3, surveyID) in enumerate(test_loader):\n",
    "\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        data3 = data3.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(data1, data2, data3)\n",
    "        predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "        # Sellect top-25 values as predictions\n",
    "        top_25 = np.argsort(-predictions, axis=1)[:, :25] \n",
    "        if top_k_indices is None:\n",
    "            top_k_indices = top_25\n",
    "        else:\n",
    "            top_k_indices = np.concatenate((top_k_indices, top_25), axis=0)\n",
    "\n",
    "        surveys.extend(surveyID.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979412c",
   "metadata": {
    "papermill": {
     "duration": 0.011846,
     "end_time": "2024-06-03T08:06:34.826829",
     "exception": false,
     "start_time": "2024-06-03T08:06:34.814983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Save prediction file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f337253",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T08:06:34.851773Z",
     "iopub.status.busy": "2024-06-03T08:06:34.851427Z",
     "iopub.status.idle": "2024-06-03T08:06:34.956110Z",
     "shell.execute_reply": "2024-06-03T08:06:34.955210Z"
    },
    "papermill": {
     "duration": 0.119628,
     "end_time": "2024-06-03T08:06:34.958253",
     "exception": false,
     "start_time": "2024-06-03T08:06:34.838625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_concatenated = [' '.join(map(str, row)) for row in top_k_indices]\n",
    "\n",
    "pd.DataFrame(\n",
    "    {'surveyId': surveys,\n",
    "     'predictions': data_concatenated,\n",
    "    }).to_csv(\"output.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6648764",
   "metadata": {
    "papermill": {
     "duration": 0.011585,
     "end_time": "2024-06-03T08:06:34.981913",
     "exception": false,
     "start_time": "2024-06-03T08:06:34.970328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8171035,
     "sourceId": 64733,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5037.316022,
   "end_time": "2024-06-03T08:06:37.603879",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-03T06:42:40.287857",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
